We observe…

1. Both training and validation RMSE steadily increase over epochs.


2. Validation RMSE remains below training RMSE at every epoch.


3. The absolute train–val RMSE gap shrinks only because both errors worsen.



This means…
The model is under-fitting its heterogeneous training set and, with an overly aggressive learning rate, is beginning to diverge rather than converge.

Solution…

Boost model capacity (add layers/neurons or relax regularization)

Align training distribution to the real operating CD/pitch/tone/direction of validation

Lower the learning rate (or adjust optimizer settings) for stable convergence



---

> Slide-ready (one line):
“Under-fitting with slight optimizer divergence: boost capacity, rebalance data, and dial back learning rate to drive both RMSE curves down.”




---

> Detailed Take-Home for Presentation:
Over epochs, both train and validation errors climb—an unusual sign of divergence driven by too-large gradient steps—and validation remains easier than the mixed training set, so its RMSE sits below the training RMSE. Although the gap narrows, it’s only because both errors worsen, not because generalization improves. To address this, we’ll increase the network’s capacity (more neurons or less regularization), refocus our training samples on the key CD/pitch regime we actually care about, and reduce the learning rate so the optimizer can steadily descend the loss landscape. These actions should lower both errors and restore a healthy train < validation relationship