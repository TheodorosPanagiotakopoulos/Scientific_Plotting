#!/bin/bash

OUTPUT_FILE="output.txt"
: > "$OUTPUT_FILE"

# Save original stdout to FD 3 for progress bar only
exec 3>&1
exec 1>>"$OUTPUT_FILE" 2>&1  # All standard output and error go to output.txt

log_files=()
log_count=0

# Enable case-insensitive globbing
shopt -s nocaseglob

# Find .log files starting with leaf or host
subdirs=( $(find . -type d -print0 | xargs -0) )
for dir in "${subdirs[@]}"; do
    for logfile in "$dir"/leaf*.log "$dir"/host*.log; do
        [[ -f "$logfile" ]] || continue
        log_files+=("$logfile")
        ((log_count++))
    done
done

# Highlight special phrases using ANSI colors
highlight_special_phrases() {
    sed -E \
        -e 's/(no such file or directory)/\x1b[0;31m\1\x1b[0m/Ig' \
        -e 's/(memory)/\x1b[0;33m\1\x1b[0m/Ig'
}

# Show progress bar to terminal only (FD 3)
show_progress() {
    local count=$1
    local total=$2
    local progress=$(( 100 * count / total ))
    local bar_width=50
    local filled=$(( progress * bar_width / 100 ))
    local bar=$(printf "%${filled}s" | tr ' ' '#')
    bar=$(printf "%-${bar_width}s" "$bar")
    printf "\rProcessing [%s]: %3d%% (%d/%d)" "$bar" "$progress" "$count" "$total" >&3
}

# Process each log file
process_logfile() {
    local logfile="$1"
    local dir
    dir=$(dirname "$logfile")

    error_tmp=$(mktemp)
    fail_tmp=$(mktemp)

    # Case-insensitive error match with awk, skip if path included
    awk '{ if (tolower($0) ~ /error/ && $0 !~ /error.*\/.*\//) print }' "$logfile" > "$error_tmp"
    grep -i 'fail' "$logfile" > "$fail_tmp"

    if [[ -s "$error_tmp" || -s "$fail_tmp" ]]; then
        echo -e "\n\nIn directory: $dir"
        echo -e " Log file: $logfile"

        if [[ -s "$error_tmp" ]]; then
            echo -e " Matches for \x1b[0;32merror\x1b[0m:"
            grep --color=always -i 'error' "$error_tmp" | highlight_special_phrases
        fi

        if [[ -s "$fail_tmp" ]]; then
            echo -e " Matches for \x1b[0;33mfail\x1b[0m:"
            grep --color=always -i 'fail' "$fail_tmp" | highlight_special_phrases
        fi
    fi

    rm -f "$error_tmp" "$fail_tmp"
}

# Loop through all log files with progress
total=${#log_files[@]}
count=0
for logfile in "${log_files[@]}"; do
    ((count++))
    show_progress "$count" "$total"
    process_logfile "$logfile"
done

# Final message
echo -e "\n\nDone. Output saved to $OUTPUT_FILE" >&3

-------


Objective

To determine which after-simulation images in the validation set are not represented in the before-simulation images in the training set â€” using two analytical methods:

1. ResNet + Cosine Similarity: captures deep, semantic similarity


2. Perceptual Hashing: detects pixel-level or structural similarity




---

ğŸ§  What Is ResNet?

ResNet (Residual Network) is a deep convolutional neural network (CNN) architecture designed to learn high-level visual features from images. It introduces residual connections (skip connections) that allow very deep networks (like ResNet-18, ResNet-50, etc.) to train effectively.

Why use ResNet here?

We use ResNet not for classification, but to extract feature embeddings from images â€” mathematical vectors that describe the semantic content (patterns, shapes, structure).

These embeddings are then used to compare images semantically â€” even if pixel-wise they differ.


---

ğŸ” What Is Semantic Similarity?

Semantic similarity measures whether two images represent the same thing, even if they are not pixel-identical.

For example, a semiconductor pattern before and after simulation might look different in brightness or alignment, but still mean the same layout or chip structure.


ResNet captures this meaning through its internal embeddings, not raw pixels.


---

ğŸ“ What Is the Definition of Similarity We Use?

We use cosine similarity between embeddings.

ğŸ”¢ Cosine Similarity Formula:

Given two vectors A and B:

cos_sim(A, B) = (A Â· B) / (||A|| * ||B||)

1.0 â†’ Perfectly similar (same direction in feature space)

0.0 â†’ Completely dissimilar

Threshold (e.g., 0.99) â†’ Cutoff to consider two images â€œthe sameâ€


âœ… Is there a built-in function?

Yes. Popular libraries like scikit-learn, PyTorch, or NumPy provide cosine similarity:

Example using scikit-learn:

from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity([embedding1], [embedding2])


---

ğŸ”¹ Method 1: Semantic Comparison Using Fine-Tuned ResNet

ğŸ§  Analytical Workflow:

1. Label Your Data

0 â†’ before simulation (training)

1 â†’ after simulation (validation)


2. Fine-Tune ResNet

Load pretrained ResNet (e.g., ResNet-18)

Replace last layer with a binary output layer

Train on labeled images to make the model learn simulation-specific features


3. Extract Embeddings

Remove the classification head after training

Pass each image through the network

Get the 512-d feature vector from the penultimate layer


4. Compare Embeddings Using Cosine Similarity

For each after-simulation (validation) image:

Compute cosine similarity to all before-simulation (training) embeddings

If max similarity < 0.99, flag as unmatched




---

âœ… Pros:

Learns high-level, domain-specific features

Works well for structural or geometric differences

Robust to brightness, noise, or small alterations


âŒ Cons:

Requires labeled data

Training time is needed

Slower than hashing



---

ğŸ”¹ Method 2: Structural Comparison Using Perceptual Hashing

ğŸ§  Analytical Workflow:

1. Generate Hashes

Use imagehash.phash() for each before-simulation image

Store results in a hash set


2. Compare Hashes

For each after-simulation image:

Generate its perceptual hash

Compare to training hashes using Hamming distance

If distance > threshold (e.g., 5), consider it unmatched



3. Interpret

Near-zero Hamming distance â†’ same visual structure

Larger distance â†’ visually different



---

âœ… Pros:

Very fast

Simple to implement

No training or labels needed


âŒ Cons:

Sensitive to image changes

Cannot detect semantic differences

Not suitable for simulation-related visual shifts


-----

Analytical Report: Identifying Unmatched After-Simulation Images Using ResNet Embeddings and Perceptual Hashing


---

ğŸ¯ Objective

To identify which after-simulation images in the validation set are not represented in the training setâ€™s after-simulation images, using two analytical approaches:

1. ResNet + Embedding Vectors + Cosine Similarity


2. Perceptual Hashing + Hamming Distance




---

ğŸ§  Dataset Structure

Each dataset (training and validation) contains:

Before-simulation images

After-simulation images


For this task, we focus only on comparing:

> After-simulation validation images vs. after-simulation training images




---

ğŸ”¹ Method 1: Semantic Comparison Using Fine-Tuned ResNet and Embeddings

âœ… Concept:

We use a ResNet model to extract a high-dimensional vector representation (embedding) of each after-simulation image. These vectors capture the semantic content â€” the structure and patterns â€” of the image.

We then compute the cosine similarity between embeddings of validation and training images. If the similarity is below a threshold, the validation image is considered unseen.


---

ğŸ§ª How Embedding Vectors Are Calculated:

1. Load a pretrained ResNet model (e.g., ResNet50)

Remove the final classification layer



2. Prepare all after-simulation images

Resize to 224Ã—224

Normalize using ImageNet mean and std



3. Pass each image through ResNet

Extract the output from the penultimate layer (typically a 2048-dimensional vector for ResNet50)



4. Result:

Each image is now represented by a fixed-length embedding vector




In TensorFlow/Keras:

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D

base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
x = GlobalAveragePooling2D()(base_model.output)
embedding_model = Model(inputs=base_model.input, outputs=x)

Then:

embedding_vector = embedding_model.predict(preprocessed_image)


---

ğŸ§ª Similarity Check Using Cosine Similarity

Once all embedding vectors are computed:

For each validation (after-simulation) image, compute cosine similarity to all training (after-simulation) embeddings.

If maximum similarity < threshold (e.g., 0.99), flag the validation image as unmatched.


Use built-in cosine similarity:

from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity([val_emb], train_embeddings)


---

âœ… Advantages:

Captures deep semantic differences

Robust to pixel-level changes (noise, contrast)

Useful when simulation introduces small visual changes


âŒ Limitations:

Requires compute and pretraining (or fine-tuning)

Slower than hash-based comparison



---

ğŸ”¹ Method 2: Structural Comparison Using Perceptual Hashing

âœ… Concept:

Generate perceptual hashes (e.g., using phash) to summarize each imageâ€™s visual fingerprint. Then compare hashes using Hamming distance.


---

ğŸ§ª Analytical Workflow:

1. Generate Hashes

Use imagehash.phash() to generate a hash (e.g., 64-bit) for each after-simulation image in both training and validation sets.



2. Compare Validation Hashes

For each validation image hash, check:

Is it present in the training hash set?

Or is the Hamming distance â‰¤ threshold (e.g., 5)?




3. Decision

If no close match found, mark as unseen.




In Python:

import imagehash
from PIL import Image

hash_val = imagehash.phash(Image.open("val/after_001.png"))
hash_train = imagehash.phash(Image.open("train/after_004.png"))
distance = hash_val - hash_train  # Hamming distance


---

âœ… Advantages:

Fast and simple

No training or GPU needed

Great for exact or near-exact duplicates


âŒ Limitations:

Fails if simulation changes layout or structure

Sensitive to transformations (rotation, resizing, noise)

